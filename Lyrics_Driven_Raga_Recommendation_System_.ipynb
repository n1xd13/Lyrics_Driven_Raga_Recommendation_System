{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n1xd13/n1xd13/blob/main/Lyrics_Driven_Raga_Recommendation_System_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPS45dw1Igq0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, f1_score, ConfusionMatrixDisplay\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything()\n",
        "np.set_printoptions(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoZObtpbI_B1"
      },
      "outputs": [],
      "source": [
        "def read_txt(filepath: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(filepath, sep=';', header=None, names=['sentence', 'label'])\n",
        "    df['label'] = df['label'].astype('category').cat.codes\n",
        "    return df\n",
        "\n",
        "train_df = read_txt(r'/content/train.txt')\n",
        "\n",
        "valid_df = read_txt(r'/content/val dataset.txt')\n",
        "test_df  = read_txt(r'/content/test dataset.txt')\n",
        "\n",
        "label_names = ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']\n",
        "\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_df['label']),  y=train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YTINXtUJh_w",
        "outputId": "9035ca7e-9eda-4213-9aab-ecfd4d63c976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkNYJC1xJ-pk",
        "outputId": "4495259a-cb35-4d64-ec5a-4574cb698fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8_GGcasJieI"
      },
      "outputs": [],
      "source": [
        "def clean_lemmatize_tokenize(text: str) -> list[str]:\n",
        "\n",
        "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')    # add/remove regex as required\n",
        "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "    NUMBERS = re.compile('\\d+')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # clean\n",
        "    text = text.lower()\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = NUMBERS.sub('', text)\n",
        "\n",
        "    # remove stopwords and lemmatize\n",
        "    tokens = [word for word in text.split() if word not in STOPWORDS]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "train_df['tokenized'] = train_df['sentence'].apply(clean_lemmatize_tokenize)\n",
        "valid_df['tokenized'] = valid_df['sentence'].apply(clean_lemmatize_tokenize)\n",
        "test_df['tokenized']  =  test_df['sentence'].apply(clean_lemmatize_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHDqP30bJigf",
        "outputId": "a593097c-7d61-4445-c02c-ab9d9e7c17b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(vocab)=3814\n",
            "len(doc_freq)=3812\n"
          ]
        }
      ],
      "source": [
        "def make_vocabulary_from_tokens(\n",
        "    tokenized_sentences: pd.Series,\n",
        "    min_doc_freq: int = 1,\n",
        "    max_doc_freq: int = 1_000_000\n",
        ") -> dict[str: int]:\n",
        "\n",
        "    # Count frequency of each token in dataset\n",
        "    document_freq = {}\n",
        "    for tokenized_sentence in tokenized_sentences:\n",
        "        for token in tokenized_sentence:\n",
        "            document_freq[token] = document_freq.get(token, 0) + 1\n",
        "\n",
        "    # Discard tokens with freq < min_doc_freq\n",
        "    qualified_tokens = {\n",
        "        token: freq for token, freq in document_freq.items() if (min_doc_freq < freq < max_doc_freq)\n",
        "    }\n",
        "\n",
        "    # Add in token_ids for each token\n",
        "    vocab = {token: token_id+2 for token_id, token in enumerate(qualified_tokens.keys())}\n",
        "\n",
        "    # Add special tokens\n",
        "    vocab['[PAD]'] = 0\n",
        "    vocab['[UNK]'] = 1\n",
        "\n",
        "    return vocab, qualified_tokens\n",
        "\n",
        "vocab, doc_freq = make_vocabulary_from_tokens(train_df['tokenized'], 3)    # use only train set for this\n",
        "\n",
        "# Use the built-in print function\n",
        "print(f'len(vocab)={len(vocab)}')\n",
        "print(f'len(doc_freq)={len(doc_freq)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9sFRdEXJiiv"
      },
      "outputs": [],
      "source": [
        "def tokens_to_input_ids(tokenized_sentence: list[str], vocabulary=vocab) -> torch.tensor:\n",
        "    input_ids = [\n",
        "        vocabulary.get(token, 1) for token in tokenized_sentence\n",
        "    ]\n",
        "    return input_ids\n",
        "\n",
        "X_train, y_train = train_df['tokenized'].apply(tokens_to_input_ids), train_df['label'].to_list()\n",
        "X_valid, y_valid = valid_df['tokenized'].apply(tokens_to_input_ids), valid_df['label'].to_list()\n",
        "X_test,  y_test  =  test_df['tokenized'].apply(tokens_to_input_ids),  test_df['label'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIjVn3dmJilA"
      },
      "outputs": [],
      "source": [
        "# Hyper parameters etc.\n",
        "class CFG:\n",
        "    n_epochs = 20\n",
        "    learning_rate = 1.0e-3\n",
        "    batch_size = 64\n",
        "    batches_per_epoch = len(X_train) // batch_size\n",
        "    label_names = [name[:3] for name in label_names]   # first few letters only for plots etc\n",
        "\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Model parameters\n",
        "        vocab_size = len(vocab)\n",
        "        n_labels = len(label_names)\n",
        "        embedding_dim = 256\n",
        "        hidden_dim = 128\n",
        "        dropout_rate = 0.25\n",
        "        lstm_dropout_rate = 0.2,\n",
        "        num_lstm_layers = 1\n",
        "\n",
        "        # Model\n",
        "        super().__init__()\n",
        "        self.n_layers = num_lstm_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim, hidden_dim, num_layers=num_lstm_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, n_labels)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embedding = self.dropout(self.embedding(X_batch))\n",
        "        hidden, carry = (\n",
        "            torch.randn(self.n_layers, len(X_batch), self.hidden_dim),\n",
        "            torch.randn(self.n_layers, len(X_batch), self.hidden_dim),\n",
        "        )\n",
        "        output, (hidden, carry) = self.lstm(embedding, (hidden, carry))\n",
        "        return self.fc(self.dropout(output[:,-1]))\n",
        "\n",
        "\n",
        "model = LSTMClassifier()\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights))   # adding class weights because unbalanced train set\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\n",
        "\n",
        "# Training performance metric\n",
        "def weighted_f1(y_true, y_pred):\n",
        "    return f1_score(y_true.argmax(1), y_pred.argmax(1), average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pAqkLirJine"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, input_ids: list[list[int]], labels: list[int]):\n",
        "        '''\n",
        "        - Stores tokenized sentences as tensors of input ids according to vocabulary mapping.\n",
        "        - Labels are directly passed as integers.\n",
        "        '''\n",
        "        self.input_ids = pad_sequence([torch.tensor(sequence) for sequence in input_ids], batch_first=True)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)  # Assuming labels are already integers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.input_ids[idx], self.labels[idx]]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-sC9wRyJip3"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    model, optimizer, metric, loss_fn, collate_fn, CFG,\n",
        "):\n",
        "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    train_loss  = []; valid_loss  = []\n",
        "    train_score = []; valid_score = []\n",
        "\n",
        "    train_loader  = DataLoader(\n",
        "        TextDataset(X_train, y_train),\n",
        "        batch_size=CFG.batch_size, collate_fn=collate_fn, shuffle=True\n",
        "    )\n",
        "    valid_loader  = DataLoader(\n",
        "        TextDataset(X_valid, y_valid),\n",
        "        batch_size=CFG.batch_size, collate_fn=collate_fn, shuffle=False\n",
        "    )\n",
        "\n",
        "    best_vloss = 1_000_000\n",
        "\n",
        "    for epoch in range(CFG.n_epochs):\n",
        "\n",
        "        # Train step\n",
        "        model.train(True)\n",
        "        running_loss = 0\n",
        "        running_score = 0\n",
        "\n",
        "        for i, data in enumerate(train_loader):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # compute metrics and store\n",
        "            score = metric(labels, outputs)\n",
        "            running_score += score\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_score.append(avg_score := float(score / (i + 1)))\n",
        "        train_loss.append(avg_loss := float(loss / (i + 1)))\n",
        "        # TODO: find way to make these numbers bigger for plotting\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        running_vloss = 0\n",
        "        running_vscore = 0\n",
        "        with torch.no_grad():\n",
        "            for i, vdata in enumerate(valid_loader):\n",
        "                vinputs, vlabels = vdata\n",
        "                voutputs = model(vinputs)\n",
        "                vloss = loss_fn(voutputs, vlabels)\n",
        "\n",
        "                vscore = metric(vlabels, voutputs)\n",
        "                running_vscore += vscore\n",
        "                running_vloss += vloss\n",
        "\n",
        "        valid_score.append(avg_vscore := float(running_vscore / (i + 1)))\n",
        "        valid_loss.append(avg_vloss := float(running_vloss / (i + 1)))\n",
        "\n",
        "\n",
        "        # Track best performance and save model's state\n",
        "        if avg_vloss < best_vloss:\n",
        "            best_vloss = avg_vloss\n",
        "            model_path = 'model_{}_{}'.format(timestamp, epoch)\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "\n",
        "        print(f'Epoch {epoch}: loss = {avg_loss} score = {avg_score} | vloss = {avg_vloss} vscore = {avg_vscore}')\n",
        "\n",
        "\n",
        "    # Plot loss and metric\n",
        "    fig,(ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "    ax1.set_xlabel('epoch'); ax1.set_ylabel('loss'); ax1.set_title('training loss')\n",
        "    ax1.plot(np.arange(len(train_loss)), train_loss, label='training')\n",
        "    ax1.plot(np.arange(len(valid_loss)), valid_loss, label='validation')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.set_xlabel('epoch'); ax2.set_ylabel('score'); ax2.set_title('training score')\n",
        "    ax2.plot(np.arange(len(train_score)), train_score, label='training')\n",
        "    ax2.plot(np.arange(len(valid_score)), valid_score, label='validation')\n",
        "    ax2.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "0QAou0dHJisO",
        "outputId": "094b3866-8a8e-444e-846a-defe1ae0dc88"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected scalar type Float but found Double",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4c222f971d94>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-10-05c245d31f53>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_valid, y_valid, model, optimizer, metric, loss_fn, collate_fn, CFG)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
          ]
        }
      ],
      "source": [
        "train_model(\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    model, optimizer, weighted_f1, loss_fn, None, CFG\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cKhuiJ2Jiuy"
      },
      "outputs": [],
      "source": [
        "def test_model(X, y, model):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        TextDataset(X, y),\n",
        "        batch_size=CFG.batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    y_preds = []\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader):\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            y_preds.append(outputs.argmax(1))\n",
        "\n",
        "    y_preds = torch.cat(y_preds)\n",
        "\n",
        "    print(classification_report(y, y_preds, target_names=label_names))\n",
        "    ConfusionMatrixDisplay.from_predictions(y, y_preds, display_labels=label_names)\n",
        "\n",
        "    return y_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZclv5LZJiw7"
      },
      "outputs": [],
      "source": [
        "vpreds = test_model(X_valid, y_valid, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zb9Mg0CJizd"
      },
      "outputs": [],
      "source": [
        "preds = test_model(X_test, y_test, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9o2XV-vV8lX"
      },
      "outputs": [],
      "source": [
        "def predict_emotion(input_text, model, vocabulary, label_names):\n",
        "    # Tokenize input text\n",
        "    tokenized_input = clean_lemmatize_tokenize(input_text)\n",
        "    input_ids = tokens_to_input_ids(tokenized_input, vocabulary)\n",
        "\n",
        "    # Convert input_ids to tensor and add batch dimension\n",
        "    input_tensor = torch.tensor(input_ids).unsqueeze(0)\n",
        "\n",
        "    # Get model prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "\n",
        "    # Get predicted emotion label\n",
        "    predicted_label = output.argmax(1).item()\n",
        "    predicted_emotion = label_names[predicted_label]\n",
        "\n",
        "    return predicted_emotion\n",
        "\n",
        "# Example usage\n",
        "input_text = \"I am feeling loved that i get to love again\"\n",
        "predicted_emotion = predict_emotion(input_text, model, vocab, CFG.label_names)\n",
        "print(f\"Predicted emotion: {predicted_emotion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN1VjepsfraL"
      },
      "outputs": [],
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bHnRvPxyqMD"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "# Function to translate text from Hindi to English\n",
        "def translate_to_english(text):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, src='hi', dest='en')\n",
        "    return translated_text.text\n",
        "\n",
        "# Assuming predict_emotion function takes English text as input\n",
        "input_text_hindi = \"मुझे तुमसे प्यार है\"\n",
        "\n",
        "# Translate input text from Hindi to English\n",
        "input_text_english = translate_to_english(input_text_hindi)\n",
        "\n",
        "# Now feed the translated text into your model\n",
        "\n",
        "predicted_emotion = predict_emotion(input_text_english, model, vocab, CFG.label_names)\n",
        "print(f\"Predicted emotion: {predicted_emotion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TCWS7yxdSXI"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "# Function to translate text from Hindi to English\n",
        "def translate_to_english(text):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, src='hi', dest='en')\n",
        "    return translated_text.text\n",
        "\n",
        "# Function to take input from the user\n",
        "def get_user_input():\n",
        "    input_text = input(\"Enter your text in Hindi: \")\n",
        "    return input_text\n",
        "\n",
        "# Get input from the user\n",
        "input_text_hindi = get_user_input()\n",
        "\n",
        "# Translate input text from Hindi to English\n",
        "input_text_english = translate_to_english(input_text_hindi)\n",
        "\n",
        "# Predict emotion based on translated text\n",
        "predicted_emotion = predict_emotion(input_text_english, model, vocab, CFG.label_names)\n",
        "print(f\"Predicted emotion: {predicted_emotion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwF4HEx_ndwf"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "# Dictionary mapping emotions to recommended ragas\n",
        "raga_recommendations = {\n",
        "    \"anger\": \"Bhairavi\",\n",
        "    \"fear\": \"Malkauns\",\n",
        "    \"joy\": \"Bilawal\",\n",
        "    \"love\": \"Yaman\",\n",
        "    \"sadness\": \"Ahir Bhairav\",\n",
        "    \"surprise\": \"Darbari Kanada\"\n",
        "}\n",
        "\n",
        "# Updated mapping between model's predicted labels and expected emotion labels\n",
        "emotion_mapping = {\n",
        "    \"ang\": \"anger\",\n",
        "    \"fear\": \"fear\",\n",
        "    \"joy\": \"joy\",\n",
        "    \"lov\": \"love\",\n",
        "    \"sad\": \"sadness\",\n",
        "    \"sur\": \"surprise\"\n",
        "}\n",
        "\n",
        "# Function to translate text from Hindi to English\n",
        "def translate_to_english(text):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, src='hi', dest='en')\n",
        "    return translated_text.text\n",
        "\n",
        "# Function to take input from the user\n",
        "def get_user_input():\n",
        "    input_text = input(\"Enter your text: \")\n",
        "    return input_text\n",
        "\n",
        "# Get input from the user\n",
        "user_input = get_user_input()\n",
        "\n",
        "# Translate input text from Hindi to English\n",
        "input_text_english = translate_to_english(user_input)\n",
        "\n",
        "# Predict emotion based on translated text\n",
        "predicted_emotion = predict_emotion(input_text_english, model, vocab, CFG.label_names)\n",
        "\n",
        "# Convert predicted emotion to lowercase and map to expected labels\n",
        "predicted_emotion_lower = predicted_emotion.lower()\n",
        "predicted_emotion_mapped = emotion_mapping.get(predicted_emotion_lower, \"Unknown\")\n",
        "\n",
        "# Get the recommended raga based on the mapped predicted emotion\n",
        "recommended_raga = raga_recommendations.get(predicted_emotion_mapped, \"Unknown\")\n",
        "\n",
        "# Display the predicted emotion and recommended raga\n",
        "print(f\"Predicted emotion: {predicted_emotion_mapped}\")\n",
        "print(f\"Recommended raga based on your emotion: {recommended_raga}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJFG72wNZGwh"
      },
      "outputs": [],
      "source": [
        "pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJu_lxMn2D_a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5NGPplwsCTH"
      },
      "outputs": [],
      "source": [
        "# prompt: !pip install --upgrade streamlit\n",
        "\n",
        "!pip install --upgrade streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93wHCCFS98dK"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6N12HsPjMIS"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "from googletrans import Translator\n",
        "\n",
        "# Dictionary mapping emotions to recommended ragas\n",
        "raga_recommendations = {\n",
        "    \"anger\": \"Bhairavi\",\n",
        "    \"fear\": \"Malkauns\",\n",
        "    \"joy\": \"Bilawal\",\n",
        "    \"love\": \"Yaman\",\n",
        "    \"sadness\": \"Ahir Bhairav\",\n",
        "    \"surprise\": \"Darbari Kanada\"\n",
        "}\n",
        "\n",
        "# Updated mapping between model's predicted labels and expected emotion labels\n",
        "emotion_mapping = {\n",
        "    \"ang\": \"anger\",\n",
        "    \"fear\": \"fear\",\n",
        "    \"joy\": \"joy\",\n",
        "    \"lov\": \"love\",\n",
        "    \"sad\": \"sadness\",\n",
        "    \"sur\": \"surprise\"\n",
        "}\n",
        "\n",
        "# Function to translate text from Hindi to English\n",
        "def translate_to_english(text):\n",
        "    translator = Translator()\n",
        "    translated_text = translator.translate(text, src='hi', dest='en')\n",
        "    return translated_text.text\n",
        "\n",
        "# Function to predict emotion (placeholder for demonstration)\n",
        "def predict_emotion(text):\n",
        "  predicted_emotion = predict_emotion(text, model, vocab, CFG.label_names)\n",
        "\n",
        "# Convert predicted emotion to lowercase and map to expected labels\n",
        "  predicted_emotion_lower = predicted_emotion.lower()\n",
        "  predicted_emotion_mapped = emotion_mapping.get(predicted_emotion_lower, \"Unknown\")\n",
        "\n",
        "# Get the recommended raga based on the mapped predicted emotion\n",
        "  recommended_raga = raga_recommendations.get(predicted_emotion_mapped, \"Unknown\")\n",
        "\n",
        "# Display the predicted emotion and recommended raga\n",
        "  print(f\"Predicted emotion: {predicted_emotion_mapped}\")\n",
        "  print(f\"Recommended raga based on your emotion: {recommended_raga}\")\n",
        "\n",
        "\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Emotion Raga Recommender\")\n",
        "\n",
        "    # Get input text from the user\n",
        "    input_text = st.text_area(\"Enter your text in Hindi:\")\n",
        "\n",
        "    # Check if input text is empty\n",
        "    if not input_text:\n",
        "        st.write(\"Please enter some text in Hindi.\")\n",
        "        return\n",
        "\n",
        "    # Translate input text from Hindi to English\n",
        "    input_text_english = translate_to_english(input_text)\n",
        "\n",
        "    # ...\n",
        "\n",
        "    if st.button(\"Predict Emotion\"):\n",
        "        # Predict emotion based on translated text\n",
        "\n",
        "        predicted_emotion = predict_emotion(input_text_english)\n",
        "\n",
        "        # Convert predicted emotion to lowercase and map to expected labels\n",
        "        predicted_emotion_mapped = emotion_mapping.get(predicted_emotion.lower(), \"Unknown\")\n",
        "\n",
        "        # Get the recommended raga based on the mapped predicted emotion\n",
        "        recommended_raga = raga_recommendations.get(predicted_emotion_mapped, \"Unknown\")\n",
        "\n",
        "        # Display the predicted emotion and recommended raga\n",
        "        st.write(f\"Predicted emotion: {predicted_emotion_mapped}\")\n",
        "        st.write(f\"Recommended raga based on your emotion: {recommended_raga}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJXw__ys-ulV"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_l9gkKx-OC4"
      },
      "outputs": [],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVCL1nfji0X8",
        "outputId": "63b82b71-b7be-4799-e911-6f348b5bbeaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: streamlit: command not found\n",
            "\u001b[K\u001b[?25h^C\n"
          ]
        }
      ],
      "source": [
        "! streamlit run keshav.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}